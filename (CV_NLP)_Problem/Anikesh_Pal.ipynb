{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anikesh_Pal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5prp79-6xz16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***All Training and Testing is performed in Google Colab***<br>\n",
        "***CSV file submitted contains data from model 1 that is from keras bold tex***t "
      ]
    },
    {
      "metadata": {
        "id": "M4tWOYNX1ZzV",
        "colab_type": "code",
        "outputId": "f77f6bf1-29a4-4e1d-f55d-510eda5c76dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K99V1FHnSn4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "1b946213-416e-44b1-aea0-54a39a94416d"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1111552\t\t    A1_024_ass1.txt\t\t     p4.py\n",
            " 815384-TAG5FG.part01.rar   a.cpp\t\t\t     p5.py\n",
            " 815384-TAG5FG.part02.rar  'Anikesh Pal Resume.pdf'\t     p6.py\n",
            " 815384-TAG5FG.part03.rar  'Anikesh Resume.gdoc'\t     p7.py\n",
            " 815384-TAG5FG.part04.rar   bnkacc.cpp\t\t\t     p8.py\n",
            " 815384-TAG5FG.part05.rar   Classroom\t\t\t     p9.py\n",
            " 815384-TAG5FG.part06.rar  'Colab Notebooks'\t\t     Pics\n",
            " 815384-TAG5FG.part07.rar  'hitkul(sample_submission).csv'   p.py\n",
            " 815384-TAG5FG.part08.rar   lists-expertise-gdoc.gdoc\t     stack1.c\n",
            " 815384-TAG5FG.part09.rar  'My Books'\t\t\t     stack2345.c\n",
            " 815384-TAG5FG.part10.rar   p10.py\t\t\t     stck.cpp\n",
            " 815384-TAG5FG.part11.rar   p11.py\t\t\t     test_image.pkl\n",
            " 815384-TAG5FG.part12.rar   p1.py\t\t\t     train_image.pkl\n",
            " 815384-TAG5FG.part13.rar   p2.py\t\t\t     train_label.pkl\n",
            " A1_019_ass3.sh\t\t    p3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eTp3sGvqJ4Iw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method 1 :- Using Keras (Deep Learning)**\n",
        "\n",
        "Train Accuracy ~ 99%<br>\n",
        "Test Accuracy ~ 80%.<br>\n",
        "Train and Test split :- 87.5%(7000 points) and 100-87.5%(1000 points) respectively.<br>\n",
        "Epochs :- 200<br>\n",
        "Two Hidden Layer has been used of dimensions 512 and 128.<br>\n",
        "Weight Initialisation is done using Random Normal by Xavier/Glorot<br>\n",
        "Activation Function Used :- ReLU and Softmax<br>\n",
        "Optimizer Used :- Adam<br>"
      ]
    },
    {
      "metadata": {
        "id": "lRnPk19-7fEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5d933e9c-6677-4ea6-d3db-8a35675109ab"
      },
      "cell_type": "code",
      "source": [
        "#Retrieving data and modifying it\n",
        "import pickle\n",
        "import numpy as np\n",
        "with open(r'drive/My Drive/train_image.pkl','rb') as f:\n",
        "    image0=pickle.load(f)\n",
        "print(len(image0))\n",
        "print(len(image0[0]))\n",
        "with open(r'drive/My Drive/train_label.pkl','rb') as f1:\n",
        "    label0=pickle.load(f1)\n",
        "print(len(label0))\n",
        "with open(r'drive/My Drive/test_image.pkl','rb') as f2:\n",
        "    image_test0=pickle.load(f2)\n",
        "print(len(image_test0))\n",
        "print(len(image_test0[0]))\n",
        "#converting Training and Test data into numpy arrays\n",
        "image=np.array(image0)\n",
        "label=np.zeros((8000,4),dtype=int)\n",
        "#One hot encoding of labels\n",
        "for i in range(0,8000):\n",
        "    if label0[i]==2:\n",
        "        label[i][0]=1\n",
        "    if label0[i]==3:\n",
        "        label[i][1]=1\n",
        "    if label0[i]==0:\n",
        "        label[i][2]=1\n",
        "    if label0[i]==6:\n",
        "        label[i][3]=1\n",
        "image_test=np.array(image_test0)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "784\n",
            "8000\n",
            "2000\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OXJvsFWHCKai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f48fe55-6800-442e-a345-c38f0a3b8e35"
      },
      "cell_type": "code",
      "source": [
        "print(image.shape)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DndjyUklwK8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalisation of Data"
      ]
    },
    {
      "metadata": {
        "id": "NNW2tecBR_1u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Normalisation/Standardisation\n",
        "image=image/255\n",
        "image_test=image_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJ3gvR3jh9hO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e559fe2-36df-4e39-9c02-febd128684a4"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "print(label[3000])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zamkM7r4iBYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Variable Initialisation\n",
        "output_dim=4\n",
        "input_dim=image.shape[1]\n",
        "batch_size=100\n",
        "np_epoch=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfU6s7ffOFOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "img1,img2,lbl1,lbl2=np.array(train_test_split(image,label,test_size=1-0.875))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxQj-mxnwQ07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Training and Fitting"
      ]
    },
    {
      "metadata": {
        "id": "wDRh7IlOveRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7990
        },
        "outputId": "6c7b16ca-a6f9-4ae5-d2fd-82bd8efb7d72"
      },
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "model_relu=Sequential()\n",
        "model_relu.add(Dense(512,activation='relu',input_shape=(input_dim,),kernel_initializer=RandomNormal(mean=0.0,stddev=0.062,seed=None)))\n",
        "model_relu.add(BatchNormalization())\n",
        "model_relu.add(Dense(128,activation='relu',kernel_initializer=RandomNormal(mean=0.0,stddev=0.125,seed=None)))\n",
        "model_relu.add(BatchNormalization())\n",
        "model_relu.add(Dense(output_dim,activation='softmax'))\n",
        "print(model_relu.summary())\n",
        "model_relu.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_relu.fit(img1,lbl1,batch_size=batch_size,epochs=np_epoch,verbose=1,validation_data=(img2,lbl2)) \n",
        "y_new=model_relu.predict_classes(image_test) \n",
        "for i in range(1500,1550):\n",
        "    if y_new[i]==0:\n",
        "        print(\"Image_test:\",i,\"-->Predicted=2\")\n",
        "    if y_new[i]==3:\n",
        "        print(\"Image_test:\",i,\"-->Predicted=6\")\n",
        "    if y_new[i]==2:\n",
        "        print(\"Image_test:\",i,\"-->Predicted=0\")\n",
        "    if y_new[i]==1:\n",
        "        print(\"Image_test:\",i,\"-->Predicted=3\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 470,660\n",
            "Trainable params: 469,380\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 7000 samples, validate on 1000 samples\n",
            "Epoch 1/200\n",
            "7000/7000 [==============================] - 4s 504us/step - loss: 0.5849 - acc: 0.7789 - val_loss: 0.4835 - val_acc: 0.8040\n",
            "Epoch 2/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.4065 - acc: 0.8399 - val_loss: 0.4979 - val_acc: 0.7940\n",
            "Epoch 3/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.3599 - acc: 0.8599 - val_loss: 0.5594 - val_acc: 0.7650\n",
            "Epoch 4/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.3346 - acc: 0.8683 - val_loss: 0.6115 - val_acc: 0.7860\n",
            "Epoch 5/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.2948 - acc: 0.8847 - val_loss: 0.6036 - val_acc: 0.7740\n",
            "Epoch 6/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.2680 - acc: 0.8964 - val_loss: 0.6307 - val_acc: 0.7870\n",
            "Epoch 7/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.2531 - acc: 0.9041 - val_loss: 0.5862 - val_acc: 0.8100\n",
            "Epoch 8/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.2498 - acc: 0.9046 - val_loss: 0.7783 - val_acc: 0.7680\n",
            "Epoch 9/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.2275 - acc: 0.9160 - val_loss: 0.5509 - val_acc: 0.8130\n",
            "Epoch 10/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.1999 - acc: 0.9271 - val_loss: 0.5540 - val_acc: 0.8130\n",
            "Epoch 11/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.1955 - acc: 0.9267 - val_loss: 0.4982 - val_acc: 0.8170\n",
            "Epoch 12/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.1761 - acc: 0.9356 - val_loss: 0.5288 - val_acc: 0.8240\n",
            "Epoch 13/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.1745 - acc: 0.9341 - val_loss: 0.7012 - val_acc: 0.7580\n",
            "Epoch 14/200\n",
            "7000/7000 [==============================] - 1s 187us/step - loss: 0.1600 - acc: 0.9387 - val_loss: 0.6407 - val_acc: 0.7870\n",
            "Epoch 15/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.1713 - acc: 0.9394 - val_loss: 0.9798 - val_acc: 0.7470\n",
            "Epoch 16/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.1516 - acc: 0.9436 - val_loss: 0.6519 - val_acc: 0.7860\n",
            "Epoch 17/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.1382 - acc: 0.9503 - val_loss: 0.5811 - val_acc: 0.8060\n",
            "Epoch 18/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.1308 - acc: 0.9537 - val_loss: 0.7752 - val_acc: 0.7680\n",
            "Epoch 19/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.1269 - acc: 0.9574 - val_loss: 0.6378 - val_acc: 0.8080\n",
            "Epoch 20/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.1105 - acc: 0.9651 - val_loss: 0.7377 - val_acc: 0.7970\n",
            "Epoch 21/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.1247 - acc: 0.9580 - val_loss: 0.6839 - val_acc: 0.8070\n",
            "Epoch 22/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.1147 - acc: 0.9600 - val_loss: 0.5966 - val_acc: 0.8290\n",
            "Epoch 23/200\n",
            "7000/7000 [==============================] - 1s 187us/step - loss: 0.1088 - acc: 0.9633 - val_loss: 0.8043 - val_acc: 0.7680\n",
            "Epoch 24/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.1053 - acc: 0.9629 - val_loss: 0.8641 - val_acc: 0.8060\n",
            "Epoch 25/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.1072 - acc: 0.9617 - val_loss: 1.0341 - val_acc: 0.7640\n",
            "Epoch 26/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.1054 - acc: 0.9623 - val_loss: 0.7122 - val_acc: 0.8030\n",
            "Epoch 27/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.1101 - acc: 0.9627 - val_loss: 1.4978 - val_acc: 0.7470\n",
            "Epoch 28/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0856 - acc: 0.9724 - val_loss: 0.9820 - val_acc: 0.7790\n",
            "Epoch 29/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0816 - acc: 0.9729 - val_loss: 0.6988 - val_acc: 0.8220\n",
            "Epoch 30/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0765 - acc: 0.9726 - val_loss: 0.7179 - val_acc: 0.8120\n",
            "Epoch 31/200\n",
            "7000/7000 [==============================] - 1s 187us/step - loss: 0.0704 - acc: 0.9769 - val_loss: 0.5969 - val_acc: 0.8260\n",
            "Epoch 32/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0539 - acc: 0.9851 - val_loss: 0.6454 - val_acc: 0.8280\n",
            "Epoch 33/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0645 - acc: 0.9791 - val_loss: 0.6996 - val_acc: 0.8140\n",
            "Epoch 34/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0742 - acc: 0.9746 - val_loss: 0.7125 - val_acc: 0.8130\n",
            "Epoch 35/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0848 - acc: 0.9701 - val_loss: 0.9223 - val_acc: 0.7870\n",
            "Epoch 36/200\n",
            "7000/7000 [==============================] - 1s 178us/step - loss: 0.0608 - acc: 0.9783 - val_loss: 0.7239 - val_acc: 0.8080\n",
            "Epoch 37/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0562 - acc: 0.9817 - val_loss: 0.7808 - val_acc: 0.8060\n",
            "Epoch 38/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0562 - acc: 0.9820 - val_loss: 0.9464 - val_acc: 0.7930\n",
            "Epoch 39/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0518 - acc: 0.9813 - val_loss: 0.6631 - val_acc: 0.8270\n",
            "Epoch 40/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0409 - acc: 0.9876 - val_loss: 0.9758 - val_acc: 0.7850\n",
            "Epoch 41/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0499 - acc: 0.9844 - val_loss: 0.9397 - val_acc: 0.7860\n",
            "Epoch 42/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0400 - acc: 0.9863 - val_loss: 0.7690 - val_acc: 0.8150\n",
            "Epoch 43/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0452 - acc: 0.9856 - val_loss: 0.7227 - val_acc: 0.8330\n",
            "Epoch 44/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0330 - acc: 0.9904 - val_loss: 0.7631 - val_acc: 0.8270\n",
            "Epoch 45/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0268 - acc: 0.9933 - val_loss: 0.7521 - val_acc: 0.8350\n",
            "Epoch 46/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0215 - acc: 0.9944 - val_loss: 0.7996 - val_acc: 0.8190\n",
            "Epoch 47/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0252 - acc: 0.9911 - val_loss: 1.0313 - val_acc: 0.8120\n",
            "Epoch 48/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0241 - acc: 0.9937 - val_loss: 0.8702 - val_acc: 0.8280\n",
            "Epoch 49/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0294 - acc: 0.9919 - val_loss: 1.1293 - val_acc: 0.7770\n",
            "Epoch 50/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0479 - acc: 0.9844 - val_loss: 0.9308 - val_acc: 0.8050\n",
            "Epoch 51/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0473 - acc: 0.9830 - val_loss: 1.0270 - val_acc: 0.7960\n",
            "Epoch 52/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0551 - acc: 0.9807 - val_loss: 0.8652 - val_acc: 0.8080\n",
            "Epoch 53/200\n",
            "7000/7000 [==============================] - 1s 186us/step - loss: 0.0444 - acc: 0.9850 - val_loss: 1.1150 - val_acc: 0.7980\n",
            "Epoch 54/200\n",
            "7000/7000 [==============================] - 1s 202us/step - loss: 0.0394 - acc: 0.9860 - val_loss: 0.9066 - val_acc: 0.8040\n",
            "Epoch 55/200\n",
            "7000/7000 [==============================] - 1s 195us/step - loss: 0.0606 - acc: 0.9800 - val_loss: 0.9058 - val_acc: 0.8010\n",
            "Epoch 56/200\n",
            "7000/7000 [==============================] - 1s 196us/step - loss: 0.0629 - acc: 0.9774 - val_loss: 1.0713 - val_acc: 0.7740\n",
            "Epoch 57/200\n",
            "7000/7000 [==============================] - 1s 197us/step - loss: 0.0710 - acc: 0.9754 - val_loss: 1.3131 - val_acc: 0.7670\n",
            "Epoch 58/200\n",
            "7000/7000 [==============================] - 1s 196us/step - loss: 0.0704 - acc: 0.9741 - val_loss: 0.9947 - val_acc: 0.8040\n",
            "Epoch 59/200\n",
            "7000/7000 [==============================] - 1s 195us/step - loss: 0.0487 - acc: 0.9830 - val_loss: 1.0339 - val_acc: 0.8100\n",
            "Epoch 60/200\n",
            "7000/7000 [==============================] - 1s 197us/step - loss: 0.0455 - acc: 0.9849 - val_loss: 1.0458 - val_acc: 0.7830\n",
            "Epoch 61/200\n",
            "7000/7000 [==============================] - 1s 188us/step - loss: 0.0402 - acc: 0.9866 - val_loss: 2.0550 - val_acc: 0.6890\n",
            "Epoch 62/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0382 - acc: 0.9869 - val_loss: 1.0870 - val_acc: 0.7940\n",
            "Epoch 63/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0425 - acc: 0.9861 - val_loss: 0.8845 - val_acc: 0.8010\n",
            "Epoch 64/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0305 - acc: 0.9893 - val_loss: 0.8526 - val_acc: 0.8160\n",
            "Epoch 65/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0345 - acc: 0.9896 - val_loss: 0.8167 - val_acc: 0.8320\n",
            "Epoch 66/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.8047 - val_acc: 0.8270\n",
            "Epoch 67/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.8240 - val_acc: 0.8380\n",
            "Epoch 68/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0227 - acc: 0.9939 - val_loss: 0.9301 - val_acc: 0.8150\n",
            "Epoch 69/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0228 - acc: 0.9923 - val_loss: 1.0496 - val_acc: 0.7830\n",
            "Epoch 70/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0241 - acc: 0.9929 - val_loss: 0.9270 - val_acc: 0.8170\n",
            "Epoch 71/200\n",
            "7000/7000 [==============================] - 1s 199us/step - loss: 0.0248 - acc: 0.9919 - val_loss: 0.9423 - val_acc: 0.8300\n",
            "Epoch 72/200\n",
            "7000/7000 [==============================] - 1s 201us/step - loss: 0.0211 - acc: 0.9943 - val_loss: 1.0304 - val_acc: 0.8250\n",
            "Epoch 73/200\n",
            "7000/7000 [==============================] - 1s 198us/step - loss: 0.0220 - acc: 0.9927 - val_loss: 0.9464 - val_acc: 0.8380\n",
            "Epoch 74/200\n",
            "7000/7000 [==============================] - 1s 200us/step - loss: 0.0230 - acc: 0.9920 - val_loss: 0.9512 - val_acc: 0.8320\n",
            "Epoch 75/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.9752 - val_acc: 0.8210\n",
            "Epoch 76/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0165 - acc: 0.9944 - val_loss: 0.9254 - val_acc: 0.8110\n",
            "Epoch 77/200\n",
            "7000/7000 [==============================] - 1s 186us/step - loss: 0.0214 - acc: 0.9941 - val_loss: 0.8907 - val_acc: 0.8120\n",
            "Epoch 78/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0143 - acc: 0.9963 - val_loss: 0.9103 - val_acc: 0.8290\n",
            "Epoch 79/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0237 - acc: 0.9921 - val_loss: 1.0562 - val_acc: 0.7990\n",
            "Epoch 80/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0409 - acc: 0.9853 - val_loss: 1.1320 - val_acc: 0.8090\n",
            "Epoch 81/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0519 - acc: 0.9814 - val_loss: 1.3891 - val_acc: 0.7560\n",
            "Epoch 82/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0661 - acc: 0.9764 - val_loss: 1.1884 - val_acc: 0.7730\n",
            "Epoch 83/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0517 - acc: 0.9833 - val_loss: 1.1934 - val_acc: 0.7900\n",
            "Epoch 84/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0282 - acc: 0.9903 - val_loss: 0.8958 - val_acc: 0.8150\n",
            "Epoch 85/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 1.0244 - val_acc: 0.8110\n",
            "Epoch 86/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 1.0195 - val_acc: 0.8130\n",
            "Epoch 87/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 1.0102 - val_acc: 0.8140\n",
            "Epoch 88/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0249 - acc: 0.9920 - val_loss: 1.0551 - val_acc: 0.8120\n",
            "Epoch 89/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0169 - acc: 0.9950 - val_loss: 1.0334 - val_acc: 0.8160\n",
            "Epoch 90/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0113 - acc: 0.9967 - val_loss: 0.8997 - val_acc: 0.8220\n",
            "Epoch 91/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0131 - acc: 0.9967 - val_loss: 1.1117 - val_acc: 0.8160\n",
            "Epoch 92/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0112 - acc: 0.9969 - val_loss: 1.0540 - val_acc: 0.8200\n",
            "Epoch 93/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 1.0197 - val_acc: 0.8220\n",
            "Epoch 94/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0198 - acc: 0.9939 - val_loss: 0.9882 - val_acc: 0.8200\n",
            "Epoch 95/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 1.0815 - val_acc: 0.8150\n",
            "Epoch 96/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0157 - acc: 0.9957 - val_loss: 1.4851 - val_acc: 0.7570\n",
            "Epoch 97/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0411 - acc: 0.9857 - val_loss: 1.0702 - val_acc: 0.8270\n",
            "Epoch 98/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0240 - acc: 0.9911 - val_loss: 1.0803 - val_acc: 0.8020\n",
            "Epoch 99/200\n",
            "7000/7000 [==============================] - 1s 178us/step - loss: 0.0177 - acc: 0.9944 - val_loss: 1.1218 - val_acc: 0.8250\n",
            "Epoch 100/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0236 - acc: 0.9921 - val_loss: 1.1737 - val_acc: 0.7900\n",
            "Epoch 101/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0262 - acc: 0.9904 - val_loss: 1.1709 - val_acc: 0.7960\n",
            "Epoch 102/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0167 - acc: 0.9947 - val_loss: 1.0299 - val_acc: 0.8400\n",
            "Epoch 103/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0157 - acc: 0.9949 - val_loss: 1.2265 - val_acc: 0.8070\n",
            "Epoch 104/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0230 - acc: 0.9923 - val_loss: 1.0835 - val_acc: 0.8090\n",
            "Epoch 105/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0446 - acc: 0.9853 - val_loss: 1.4534 - val_acc: 0.7760\n",
            "Epoch 106/200\n",
            "7000/7000 [==============================] - 1s 175us/step - loss: 0.0382 - acc: 0.9871 - val_loss: 1.2149 - val_acc: 0.8080\n",
            "Epoch 107/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0457 - acc: 0.9850 - val_loss: 1.1650 - val_acc: 0.7950\n",
            "Epoch 108/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0359 - acc: 0.9871 - val_loss: 1.0649 - val_acc: 0.8160\n",
            "Epoch 109/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0204 - acc: 0.9946 - val_loss: 1.0331 - val_acc: 0.8140\n",
            "Epoch 110/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.9865 - val_acc: 0.8210\n",
            "Epoch 111/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0080 - acc: 0.9981 - val_loss: 1.0421 - val_acc: 0.8100\n",
            "Epoch 112/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.9379 - val_acc: 0.8270\n",
            "Epoch 113/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0113 - acc: 0.9967 - val_loss: 1.0199 - val_acc: 0.8130\n",
            "Epoch 114/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 1.1145 - val_acc: 0.7960\n",
            "Epoch 115/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0192 - acc: 0.9943 - val_loss: 1.1121 - val_acc: 0.8030\n",
            "Epoch 116/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0108 - acc: 0.9973 - val_loss: 1.1406 - val_acc: 0.8200\n",
            "Epoch 117/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0154 - acc: 0.9953 - val_loss: 1.0535 - val_acc: 0.8140\n",
            "Epoch 118/200\n",
            "7000/7000 [==============================] - 1s 186us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 1.0626 - val_acc: 0.8140\n",
            "Epoch 119/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 1.0097 - val_acc: 0.8200\n",
            "Epoch 120/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0105 - acc: 0.9969 - val_loss: 1.0901 - val_acc: 0.7880\n",
            "Epoch 121/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0170 - acc: 0.9934 - val_loss: 1.2083 - val_acc: 0.8000\n",
            "Epoch 122/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 1.3572 - val_acc: 0.7920\n",
            "Epoch 123/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0224 - acc: 0.9931 - val_loss: 1.3162 - val_acc: 0.7890\n",
            "Epoch 124/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0269 - acc: 0.9913 - val_loss: 1.2228 - val_acc: 0.7860\n",
            "Epoch 125/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0350 - acc: 0.9879 - val_loss: 1.1096 - val_acc: 0.8060\n",
            "Epoch 126/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0354 - acc: 0.9871 - val_loss: 1.4100 - val_acc: 0.8060\n",
            "Epoch 127/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0376 - acc: 0.9861 - val_loss: 1.1611 - val_acc: 0.8030\n",
            "Epoch 128/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0236 - acc: 0.9920 - val_loss: 1.0326 - val_acc: 0.8300\n",
            "Epoch 129/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 1.1175 - val_acc: 0.8120\n",
            "Epoch 130/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0187 - acc: 0.9944 - val_loss: 1.0602 - val_acc: 0.8220\n",
            "Epoch 131/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 1.0556 - val_acc: 0.8170\n",
            "Epoch 132/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 1.0510 - val_acc: 0.8240\n",
            "Epoch 133/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 1.0357 - val_acc: 0.8260\n",
            "Epoch 134/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1154 - val_acc: 0.8100\n",
            "Epoch 135/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0054 - acc: 0.9987 - val_loss: 1.0971 - val_acc: 0.8290\n",
            "Epoch 136/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0090 - acc: 0.9986 - val_loss: 1.0392 - val_acc: 0.8100\n",
            "Epoch 137/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.9841 - val_acc: 0.8380\n",
            "Epoch 138/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 1.0622 - val_acc: 0.8230\n",
            "Epoch 139/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.9981 - val_acc: 0.8280\n",
            "Epoch 140/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 1.0059 - val_acc: 0.8350\n",
            "Epoch 141/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0048 - acc: 0.9986 - val_loss: 1.1462 - val_acc: 0.8060\n",
            "Epoch 142/200\n",
            "7000/7000 [==============================] - 1s 186us/step - loss: 0.0068 - acc: 0.9984 - val_loss: 1.1595 - val_acc: 0.8240\n",
            "Epoch 143/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0105 - acc: 0.9963 - val_loss: 1.2350 - val_acc: 0.8170\n",
            "Epoch 144/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0213 - acc: 0.9920 - val_loss: 1.1983 - val_acc: 0.8210\n",
            "Epoch 145/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0258 - acc: 0.9916 - val_loss: 1.3040 - val_acc: 0.8070\n",
            "Epoch 146/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0355 - acc: 0.9864 - val_loss: 1.3142 - val_acc: 0.7980\n",
            "Epoch 147/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0561 - acc: 0.9811 - val_loss: 1.1431 - val_acc: 0.7810\n",
            "Epoch 148/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0809 - acc: 0.9716 - val_loss: 1.3367 - val_acc: 0.7740\n",
            "Epoch 149/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0880 - acc: 0.9707 - val_loss: 1.0235 - val_acc: 0.8300\n",
            "Epoch 150/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0426 - acc: 0.9840 - val_loss: 1.1862 - val_acc: 0.8070\n",
            "Epoch 151/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0269 - acc: 0.9904 - val_loss: 0.9936 - val_acc: 0.8300\n",
            "Epoch 152/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0167 - acc: 0.9937 - val_loss: 1.0165 - val_acc: 0.8300\n",
            "Epoch 153/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0177 - acc: 0.9950 - val_loss: 1.0343 - val_acc: 0.8250\n",
            "Epoch 154/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0114 - acc: 0.9967 - val_loss: 1.0324 - val_acc: 0.8320\n",
            "Epoch 155/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 1.0283 - val_acc: 0.8220\n",
            "Epoch 156/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0104 - acc: 0.9964 - val_loss: 1.0888 - val_acc: 0.8250\n",
            "Epoch 157/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 1.0848 - val_acc: 0.8260\n",
            "Epoch 158/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0085 - acc: 0.9979 - val_loss: 1.1521 - val_acc: 0.8180\n",
            "Epoch 159/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0199 - acc: 0.9931 - val_loss: 1.2265 - val_acc: 0.7910\n",
            "Epoch 160/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0119 - acc: 0.9967 - val_loss: 1.0908 - val_acc: 0.8250\n",
            "Epoch 161/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 1.2243 - val_acc: 0.8080\n",
            "Epoch 162/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 1.0405 - val_acc: 0.8280\n",
            "Epoch 163/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 1.0736 - val_acc: 0.8150\n",
            "Epoch 164/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0067 - acc: 0.9987 - val_loss: 1.0290 - val_acc: 0.8220\n",
            "Epoch 165/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0053 - acc: 0.9986 - val_loss: 1.0560 - val_acc: 0.8310\n",
            "Epoch 166/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 1.0027 - val_acc: 0.8380\n",
            "Epoch 167/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 1.0674 - val_acc: 0.8290\n",
            "Epoch 168/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 1.0380 - val_acc: 0.8300\n",
            "Epoch 169/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.0748 - val_acc: 0.8380\n",
            "Epoch 170/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 1.1196 - val_acc: 0.8320\n",
            "Epoch 171/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 1.0969 - val_acc: 0.8280\n",
            "Epoch 172/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 1.1490 - val_acc: 0.8150\n",
            "Epoch 173/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 1.0903 - val_acc: 0.8290\n",
            "Epoch 174/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0134 - acc: 0.9954 - val_loss: 1.2894 - val_acc: 0.8110\n",
            "Epoch 175/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0191 - acc: 0.9944 - val_loss: 1.5573 - val_acc: 0.7650\n",
            "Epoch 176/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0281 - acc: 0.9906 - val_loss: 1.2392 - val_acc: 0.7930\n",
            "Epoch 177/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0363 - acc: 0.9880 - val_loss: 1.2567 - val_acc: 0.8000\n",
            "Epoch 178/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0226 - acc: 0.9929 - val_loss: 1.1627 - val_acc: 0.8230\n",
            "Epoch 179/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0148 - acc: 0.9949 - val_loss: 1.2429 - val_acc: 0.8180\n",
            "Epoch 180/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0182 - acc: 0.9937 - val_loss: 1.2241 - val_acc: 0.8040\n",
            "Epoch 181/200\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.0183 - acc: 0.9940 - val_loss: 1.2890 - val_acc: 0.8050\n",
            "Epoch 182/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0132 - acc: 0.9951 - val_loss: 1.3248 - val_acc: 0.8080\n",
            "Epoch 183/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 1.1906 - val_acc: 0.8150\n",
            "Epoch 184/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0102 - acc: 0.9971 - val_loss: 1.1545 - val_acc: 0.8180\n",
            "Epoch 185/200\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 1.2134 - val_acc: 0.7980\n",
            "Epoch 186/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0151 - acc: 0.9946 - val_loss: 1.3079 - val_acc: 0.8030\n",
            "Epoch 187/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0229 - acc: 0.9920 - val_loss: 1.0568 - val_acc: 0.8060\n",
            "Epoch 188/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0158 - acc: 0.9943 - val_loss: 1.3836 - val_acc: 0.8010\n",
            "Epoch 189/200\n",
            "7000/7000 [==============================] - 1s 187us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 1.2078 - val_acc: 0.8220\n",
            "Epoch 190/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0107 - acc: 0.9960 - val_loss: 1.1548 - val_acc: 0.8180\n",
            "Epoch 191/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0088 - acc: 0.9979 - val_loss: 1.2556 - val_acc: 0.8110\n",
            "Epoch 192/200\n",
            "7000/7000 [==============================] - 1s 179us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 1.3996 - val_acc: 0.7930\n",
            "Epoch 193/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0099 - acc: 0.9974 - val_loss: 1.1486 - val_acc: 0.8090\n",
            "Epoch 194/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 1.2216 - val_acc: 0.7920\n",
            "Epoch 195/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0115 - acc: 0.9956 - val_loss: 1.1945 - val_acc: 0.8110\n",
            "Epoch 196/200\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.0066 - acc: 0.9984 - val_loss: 1.1050 - val_acc: 0.8290\n",
            "Epoch 197/200\n",
            "7000/7000 [==============================] - 1s 180us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 1.0479 - val_acc: 0.8360\n",
            "Epoch 198/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0055 - acc: 0.9980 - val_loss: 1.1675 - val_acc: 0.8200\n",
            "Epoch 199/200\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 1.2514 - val_acc: 0.8040\n",
            "Epoch 200/200\n",
            "7000/7000 [==============================] - 1s 181us/step - loss: 0.0266 - acc: 0.9906 - val_loss: 1.2705 - val_acc: 0.8150\n",
            "Image_test: 1500 -->Predicted=6\n",
            "Image_test: 1501 -->Predicted=6\n",
            "Image_test: 1502 -->Predicted=6\n",
            "Image_test: 1503 -->Predicted=0\n",
            "Image_test: 1504 -->Predicted=6\n",
            "Image_test: 1505 -->Predicted=6\n",
            "Image_test: 1506 -->Predicted=2\n",
            "Image_test: 1507 -->Predicted=6\n",
            "Image_test: 1508 -->Predicted=6\n",
            "Image_test: 1509 -->Predicted=6\n",
            "Image_test: 1510 -->Predicted=6\n",
            "Image_test: 1511 -->Predicted=6\n",
            "Image_test: 1512 -->Predicted=3\n",
            "Image_test: 1513 -->Predicted=0\n",
            "Image_test: 1514 -->Predicted=6\n",
            "Image_test: 1515 -->Predicted=6\n",
            "Image_test: 1516 -->Predicted=6\n",
            "Image_test: 1517 -->Predicted=6\n",
            "Image_test: 1518 -->Predicted=6\n",
            "Image_test: 1519 -->Predicted=2\n",
            "Image_test: 1520 -->Predicted=6\n",
            "Image_test: 1521 -->Predicted=6\n",
            "Image_test: 1522 -->Predicted=6\n",
            "Image_test: 1523 -->Predicted=6\n",
            "Image_test: 1524 -->Predicted=2\n",
            "Image_test: 1525 -->Predicted=6\n",
            "Image_test: 1526 -->Predicted=6\n",
            "Image_test: 1527 -->Predicted=6\n",
            "Image_test: 1528 -->Predicted=6\n",
            "Image_test: 1529 -->Predicted=6\n",
            "Image_test: 1530 -->Predicted=6\n",
            "Image_test: 1531 -->Predicted=6\n",
            "Image_test: 1532 -->Predicted=6\n",
            "Image_test: 1533 -->Predicted=0\n",
            "Image_test: 1534 -->Predicted=6\n",
            "Image_test: 1535 -->Predicted=6\n",
            "Image_test: 1536 -->Predicted=6\n",
            "Image_test: 1537 -->Predicted=6\n",
            "Image_test: 1538 -->Predicted=6\n",
            "Image_test: 1539 -->Predicted=3\n",
            "Image_test: 1540 -->Predicted=6\n",
            "Image_test: 1541 -->Predicted=6\n",
            "Image_test: 1542 -->Predicted=6\n",
            "Image_test: 1543 -->Predicted=2\n",
            "Image_test: 1544 -->Predicted=6\n",
            "Image_test: 1545 -->Predicted=6\n",
            "Image_test: 1546 -->Predicted=6\n",
            "Image_test: 1547 -->Predicted=6\n",
            "Image_test: 1548 -->Predicted=2\n",
            "Image_test: 1549 -->Predicted=6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4WRdWI2xwXiT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adam Optimizer is used here as iit gives adaptive learning rate giving better results"
      ]
    },
    {
      "metadata": {
        "id": "eLcf2Qe3K0Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating CSV File \n",
        "import pandas as pd\n",
        "lst=[]\n",
        "for i in range(2000):\n",
        "    if y_new[i]==0:\n",
        "        lst.append(2)\n",
        "    if y_new[i]==3:\n",
        "        lst.append(6)\n",
        "    if y_new[i]==2:\n",
        "        lst.append(0)\n",
        "    if y_new[i]==1:\n",
        "        lst.append(3)\n",
        "d={'Test_image_index':range(len(lst)),'predicted class':lst}\n",
        "df=pd.DataFrame(d)\n",
        "df.to_csv('Anikesh_Pal.csv',encoding='utf-8',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6TY2nA2szl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('Anikesh_Pal.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XMLMOS_RxrE_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Count of each class label"
      ]
    },
    {
      "metadata": {
        "id": "YLS46sa1n9Wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "58d673d3-b001-4a15-c8c3-15288a0d005d"
      },
      "cell_type": "code",
      "source": [
        "#Counting Class Labels\n",
        "print(lst.count(0))\n",
        "print(lst.count(2))\n",
        "print(lst.count(3))\n",
        "print(lst.count(6))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "515\n",
            "509\n",
            "462\n",
            "514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Eo_7q7Nez90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method 2 :- Using TensorFlow (Deep Learning)**\n",
        "\n",
        "Accuracy :- 83.30%.<br>\n",
        "Train and Test split :- 87.5%(7000 points) and 100-87.5%(1000 points) respectively.<br>\n",
        "Epochs :- 200<br>\n",
        "Two Hidden Layer has been used of dimensions 512 and 128.<br>\n",
        "Weight Initialisation is done using Random Normal by Xavier/Glorot<br>\n",
        "Activation Function Used :- Sigmoid and Softmax<br>\n",
        "Optimizer Used :- Adam<br>"
      ]
    },
    {
      "metadata": {
        "id": "I0C_Z0yrET72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d87129d8-713e-4b5b-e969-58af10a78b21"
      },
      "cell_type": "code",
      "source": [
        "#Data Retrieval\n",
        "import pickle\n",
        "import numpy as np\n",
        "with open(r'drive/My Drive/train_image.pkl','rb') as f:\n",
        "    image0=pickle.load(f)\n",
        "print(len(image0))\n",
        "print(len(image0[0]))\n",
        "with open(r'drive/My Drive/train_label.pkl','rb') as f1:\n",
        "    label0=pickle.load(f1)\n",
        "print(len(label0))\n",
        "with open(r'drive/My Drive/test_image.pkl','rb') as f2:\n",
        "    image_test0=pickle.load(f2)\n",
        "print(len(image_test0))\n",
        "print(len(image_test0[0]))\n",
        "#COnverting data to numpy arrays and labels to one hot encoded numpy array\n",
        "image=np.array(image0)\n",
        "label=np.zeros((8000,4),dtype=int)\n",
        "for i in range(0,8000):\n",
        "    if label0[i]==2:\n",
        "        label[i][0]=1\n",
        "    if label0[i]==3:\n",
        "        label[i][1]=1\n",
        "    if label0[i]==0:\n",
        "        label[i][2]=1\n",
        "    if label0[i]==6:\n",
        "        label[i][3]=1\n",
        "image_test=np.array(image_test0)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "784\n",
            "8000\n",
            "2000\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A136OI5qe_hr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "img1,img2,lbl1,lbl2=np.array(train_test_split(image,label,test_size=1-0.875))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ju4jLbOKEUtl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_hidden_1=512\n",
        "n_hidden_2=128\n",
        "n_input=784\n",
        "n_classes=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sld-bjBVwxvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weight and Bias Initialization"
      ]
    },
    {
      "metadata": {
        "id": "_-1JfWosEXc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "training_epochs=200\n",
        "learning_rate=0.001\n",
        "batch_size=100\n",
        "x=tf.placeholder(tf.float32,[None,784])\n",
        "y_=tf.placeholder(tf.float32,[None,4])\n",
        "#Initialising Weights and biases\n",
        "weights_relu={\n",
        "    'h1':tf.Variable(tf.random_normal([n_input,n_hidden_1],stddev=0.062,mean=0)),\n",
        "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2],stddev=0.125,mean=0)),\n",
        "    'out':tf.Variable(tf.random_normal([n_hidden_2,n_classes],stddev=0.120,mean=0))\n",
        "}\n",
        "biases={\n",
        "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArLHi77Aw46r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining Multiple Perceptron Layer"
      ]
    },
    {
      "metadata": {
        "id": "Az_1kF8xEcVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining Layers\n",
        "def multilayer_perceptron(x,weights,biases):\n",
        "    print('x:',x.get_shape(),'W[h1]:',weights['h1'].get_shape(),'b[h1]:',biases['b1'].get_shape())\n",
        "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
        "    layer_1=tf.nn.sigmoid(layer_1)\n",
        "    print('layer_1:',layer_1.get_shape(),'W[h2]:',weights['h2'].get_shape(),'b[h2]:',biases['b2'].get_shape())\n",
        "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
        "    layer_2=tf.nn.sigmoid(layer_2)\n",
        "    print('layer_2:',layer_2.get_shape(),'W[out]:',weights['out'].get_shape(),'b3:',biases['out'].get_shape())\n",
        "    out_layer=tf.matmul(layer_2, weights['out'])+biases['out']   \n",
        "    out_layer=tf.nn.sigmoid(out_layer)\n",
        "    print('out_layer:',out_layer.get_shape())\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypRFVSs3w-xq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Trainning and prediction"
      ]
    },
    {
      "metadata": {
        "id": "fqzbD2LaEdTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f34d6aac-e4a8-4c3e-a21a-d2cf40fe505d"
      },
      "cell_type": "code",
      "source": [
        "y_adam=multilayer_perceptron(x,weights_relu,biases)\n",
        "cost_adam=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_adam,labels=y_))\n",
        "optimizer_adam=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_adam)\n",
        "total_batch=int(len(image)/batch_size)\n",
        "with tf.Session() as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    for epoch in range(training_epochs):\n",
        "        sess.run([optimizer_adam,cost_adam,weights_relu],feed_dict={x:img1,y_:lbl1})\n",
        "    lstx=sess.run(tf.argmax(y_adam,1),feed_dict={x:image_test})\n",
        "    correct_prediction=tf.equal(tf.argmax(y_adam,1),tf.argmax(y_,1))\n",
        "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(\"Accuracy:\", accuracy.eval({x:img2,y_:lbl2}))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W[h1]: (784, 512) b[h1]: (512,)\n",
            "layer_1: (?, 512) W[h2]: (512, 128) b[h2]: (128,)\n",
            "layer_2: (?, 128) W[out]: (128, 4) b3: (4,)\n",
            "out_layer: (?, 4)\n",
            "Accuracy: 0.833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hQRwm-nAFIDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstx1=[]\n",
        "for i in range(len(lstx)):\n",
        "    if lstx[i]==2:\n",
        "        lstx1.append(0)\n",
        "    if lstx[i]==3:\n",
        "        lstx1.append(6)\n",
        "    if lstx[i]==1:\n",
        "        lstx1.append(3)\n",
        "    if lstx[i]==0:\n",
        "        lstx1.append(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2DOfEPbxm4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Count of each class label"
      ]
    },
    {
      "metadata": {
        "id": "yc2blQ_mkSOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c90e08e0-04d8-4c0d-e931-5726dba11ba7"
      },
      "cell_type": "code",
      "source": [
        "print(lstx1.count(0))\n",
        "print(lstx1.count(2))\n",
        "print(lstx1.count(3))\n",
        "print(lstx1.count(6))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "499\n",
            "534\n",
            "506\n",
            "461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xk95lSe0mUWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method 3 :- Using Logistic Regression**\n",
        "\n",
        "Accuracy :- 80.40%.<br>\n",
        "Train and Test split :- 87.5%(7000 points) and 100-87.5%(1000 points) respectively.<br>\n",
        "Epochs :- 500<br>\n",
        "Solver Used :- LBFGS"
      ]
    },
    {
      "metadata": {
        "id": "Gd7jBdJwmbWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "75aa90f1-e5e6-4d1e-fae6-d78a7b188340"
      },
      "cell_type": "code",
      "source": [
        "#Data Retreival\n",
        "import pickle\n",
        "import numpy as np\n",
        "with open(r'drive/My Drive/train_image.pkl','rb') as f:\n",
        "    image0=pickle.load(f)\n",
        "print(len(image0))\n",
        "print(len(image0[0]))\n",
        "with open(r'drive/My Drive/train_label.pkl','rb') as f1:\n",
        "    label0=pickle.load(f1)\n",
        "print(len(label0))\n",
        "with open(r'drive/My Drive/test_image.pkl','rb') as f2:\n",
        "    image_test0=pickle.load(f2)\n",
        "print(len(image_test0))\n",
        "print(len(image_test0[0]))\n",
        "#Converting data to numpy array\n",
        "image=np.array(image0)\n",
        "label=np.array(label0)\n",
        "image_test=np.array(image_test0)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "784\n",
            "8000\n",
            "2000\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-irY74UVmcQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd97846a-9e5d-46c5-c318-c0df64651dc5"
      },
      "cell_type": "code",
      "source": [
        "print(label.shape)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m9XnezfcxOBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalisation of data"
      ]
    },
    {
      "metadata": {
        "id": "N2d5ghfJmfyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image=image/255\n",
        "image_test=image_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xMRh8PAfxRnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Splitting into training and testing for checking accuracy"
      ]
    },
    {
      "metadata": {
        "id": "74rhS2w4mlCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "img1,img2,lbl1,lbl2=np.array(train_test_split(image,label,test_size=1-0.875))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zlnXVuvfxZYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Training and Predicting"
      ]
    },
    {
      "metadata": {
        "id": "Wgc0B0VDmlKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "58ed8201-46e4-4799-d846-1dda2f094254"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logReg=LogisticRegression(solver='lbfgs',max_iter=500)\n",
        "logReg.fit(img1,lbl1)\n",
        "lst=logReg.predict(image_test)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uh-8b0J7xede",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy calculation"
      ]
    },
    {
      "metadata": {
        "id": "JxHgY0OZmlaq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8fcf0ab-fb8a-4e5d-ffbf-5dee3e813563"
      },
      "cell_type": "code",
      "source": [
        "print(logReg.score(img2,lbl2))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n1V-WFtdmx-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "image_test_pd=pd.DataFrame(image_test)\n",
        "lst1= list(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnH86eZgxior",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Count of each class label"
      ]
    },
    {
      "metadata": {
        "id": "cHdwoysUm1bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a8a22e84-8767-43ed-b6da-ff5990724ad3"
      },
      "cell_type": "code",
      "source": [
        "print(lst1.count(0))\n",
        "print(lst1.count(2))\n",
        "print(lst1.count(3))\n",
        "print(lst1.count(6))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "518\n",
            "538\n",
            "500\n",
            "444\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}